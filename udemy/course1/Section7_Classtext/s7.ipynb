{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(folder + training.1600000.oricessed.noemoticon.csv\", encoding=\"latin\", header=None)\n",
    "df.head()\n",
    "df.iloc[:,[0,5]]\n",
    "df.columns = ['sentiment', 'text']\n",
    "df.head()\n",
    "df['sentiment'].value_counts()\n",
    "sents = {0: 'negative', 4: 'positive'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label(label):\n",
    "\treturn sents[label]\n",
    "\n",
    "df['sentiment'].replace({0: 'negative', 4: 'positive'}, inplace=True)\n",
    "#df['sentiment'] = df['sentiment'].apply(lambda x: change_label(x))\n",
    "df.head()\n",
    "df.samples(10) # 10 random sampless (tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage et pretraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "#regrouper les mots par racines (sens)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "\t# remove urls / mentions / and not alphanumeric characters\n",
    "\ttext_cleaning_regex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\ttext = re.sub(text_cleaning_regex, '', str(text).lower().strip())\n",
    "\t# remove stopwords\n",
    "\ttokens = []\n",
    "\tfor word in text.split(\" \"):\n",
    "\t\tif word not in english_stopwords:\n",
    "\t\t\ttokens.append(word)\n",
    "\tcleaned_text = \" \".join(tokens)\n",
    "\treturn cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization et Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_dta = train_test-split(df, test_size = 0.2, stratify=df['sentiment'], random_state=42)\n",
    "len(train_data), len(test_data)\n",
    "\n",
    "test_data['sentiment'].value_counts()\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) #248 676 trop grand \n",
    "#vocab_size = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoder et Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "training_labels = encoder.fit_transform(train_data['sentiment'])\n",
    "test_labels = encoder.transform(test_data['sentiment'])\n",
    "\n",
    "training_labels = training_labels.reshape(-1,1)\n",
    "test_labels = test_labels.reshape(-1,1)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "n.power(70000,1/4)\n",
    "# modelling\n",
    "model = tf.keras.models.Sequential(\n",
    "\t[\n",
    "\t\ttf.keras.layers.Embedding(vocab_size, 16),\n",
    "\t\ttf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "\t\ttf.keras.layers.Dense(68, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\t]\n",
    ")\n",
    "model.compile\n",
    "model_ckp = tf.keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True)\n",
    "stop = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "h = model.fit(trainig_padded, training_labels, validation_data=(test_padded, test_labels), epochs=10, callbacks=[model_ckp, stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser une nouvelle layer recurrente \n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "\t[\n",
    "\t\ttf.keras.layers.Embedding(vocab_size, 16),\n",
    "\t\ttf.keras.layers.SimpleRNN(10), \t\t\t\t\t# a simple RNN layer\n",
    "\t\ttf.keras.layers.Dense(68, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\t]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "# a recompiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il faut utiliser un LSTM pour eviter le probleme de vanishing gradient\n",
    "# utiliser un LSTM (Bidirectional)\n",
    "model = tf.keras.models.Sequential(\n",
    "\t[\n",
    "\t\ttf.keras.layers.Embedding(vocab_size, 16),\n",
    "\t\ttf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16), return_sequences=True),\n",
    "\t\ttf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
    "\t\ttf.keras.layers.Dense(68, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\t]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "#model.compile()\n",
    "#h = model.fit()\n",
    "#changer l'optimizer, l'architecture et le. learning rate\n",
    "#adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#rajouter du dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
