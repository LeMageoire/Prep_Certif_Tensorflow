{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_sentences = []\n",
    "imdb_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "train_data = tfds.as_numpy(tfds.load('imdb_reviews', split='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_data:\n",
    "\timdb_sentences.append(str(item['text']))\n",
    "\timdb_labels.append(item['label'])\n",
    "\n",
    "len(imdb_sentences), len(imdb_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = imdb_sentences[:20000]\n",
    "test_sentences = imdb_sentences[20000:]\n",
    "\n",
    "training_labels = imdb_labels[:20000]\n",
    "test_labels = imdb_labels[20000:]\n",
    "\n",
    "len(training_sentences), len(test_sentences), len(training_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences\n",
    "phrase = \"Je suis au marché\"\n",
    "tokens = phrase.split(' ')\n",
    "# tout les mots sont séparés par un espace\n",
    "# tout en miniscule \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_sentences) \n",
    "#tokenizer.word_index\n",
    "#tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(training_sentences)\n",
    "# le probleme est que des mots dans le test set n'existe pas dans le training set\n",
    "# notion de OOV (Out Of Vocabulary)\n",
    "tokenizer2 = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "tokenizer2.fit_on_texts(training_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding et Truncating du texte\n",
    "phrases = [ ]\n",
    "tokenizer = Tokenizer(num_words = 100,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# default tout les zeros a gauches (les phrases font la meme taille), prepadding\n",
    "# padding='post' pour postpadding\n",
    "# maxlen=5 pour limiter la taille des phrases\n",
    "# truncating='post' pour tronquer les phrases a la fin\n",
    "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les stopwords sont des mots qui n'ont pas de sens (the, a, an, in, on, ...) = dead words (mots vides)\n",
    "# je vais au marche le Jeudi => je vais marche Jeudi\n",
    "\n",
    "stopwords = ['au', \"le\"]\n",
    "words = phrase.split(' ')\n",
    "phrase_cleaned = \"\"\n",
    "\n",
    "for word in words:\n",
    "\tif word not in stopwords:\n",
    "\t\tphrase_cleaned.append(word)\n",
    "print(''.join(phrases_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser package NLTK pour les stopwords\n",
    "# on a un tableau numpy\n",
    "# calcul du sentiment\n",
    "# chaque mot a un poids (positif ou negatif), on fait la somme des poids\n",
    "# si positif => sentiment positif\n",
    "# si negatif => sentiment negatif\n",
    "# difficile pour evaluer le sarcasme (+1 et -1 = Neutre)\n",
    "# autre strategie : plusieurs poids pour chaque mot [categories, sentiment, nombre de lettres, occurance]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les Embeddings - Intuition\n",
    "vocab_size, em\n",
    "\n",
    "plusieurs poids par mot\n",
    "classification binaire compare au label\n",
    "\n",
    "example :\n",
    "Royaute + Homme = Roi\n",
    "Roi - Homme + Femme = Reine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embedding_dim = 10000, 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training \n",
    "training_sentences[42] \t#un commentaire\n",
    "training_labels[42] \t#label du commentaire\n",
    "#nettoyage du texte\n",
    "tokenizer = Tokenizer(num_words = 20000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "#len(tokenizer.word_index) #nombre de mots dans le dictionnaire = too much reduire a 20000\n",
    "training_sentences = tokenizer.texts_to_sequences(training_sentences)\n",
    "#on choisi 15 mots par phrase \n",
    "training_padded = pad_sequences(training_sentences, maxlen=15, padding='post', truncating='post')\n",
    "\n",
    "test_sentences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(tokenizer.texts_to_sequences(test_sentences), maxlen=15, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy array\n",
    "import numpy as np \n",
    "\n",
    "training_labels = np.array(training_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequentials(\n",
    "\t[\n",
    "\t\ttf.keras.layers.Embedding(20000, 20),\n",
    "\t\ttf.keras.layers.GlobalAveragePooling1D(),\n",
    "\t\ttf.keras.layers.Dense(8,activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\t])\n",
    "\n",
    "# model.summary() donne le total de parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckp = tf.keras.callbacks.ModelCheckpoint(filepath = 'model.h5',\n",
    "\t\t\t\t\t\t\t\t\tmonitor=\"val_accuracy\",\n",
    "\t\t\t\t\t\t\t\t\tmode=\"max\",\n",
    "\t\t\t\t\t\t\t\t\tsave_best_only=True)\n",
    "stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "\t\t\t\t\t\t\t\t\t\tpatience=3,\n",
    "\t\t\t\t\t\t\t\t\t\trestore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(training_padded, training_labels,\n",
    "\t\t\t  epochs=50,\n",
    "\t\t\t  validation_data=(test_padded, test_labels),\n",
    "\t\t\t  callbacks=[model_ckp, stop])\n",
    "# ajouter un callback pour arreter l'entrainement si la validation loss ne diminue pas\n",
    "# ajouter un callback pour sauvegarder le meilleur model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot le graph de l'accuracy et de la loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "\tplt.plot(history.history[string])\n",
    "\tplt.plot(history.history['val_'+string])\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.ylabel(string)\n",
    "\tplt.legend([string, 'val_'+string])\n",
    "\tplt.show()\n",
    "\n",
    "plot_graphs(h, 'accuracy')\n",
    "plot_graphs(h, 'loss')\n",
    "# on peut voir que le model commence a overfitting \n",
    "# training progress but validation progress is not good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Astuces pour ammeliorer le model\n",
    "#1. Augmenter le nombre de mots dans le dictionnaire\n",
    "# si vocab_size est trop grand, pas assez d'entrainements (mot inutiles)\n",
    "# check la frequence des mots\n",
    "wc = tokenizer.word_counts\n",
    "wc = sorted(wc.items(), key=lambda x:x[1], reverse=True) \n",
    "# prends les mots les plus frequents \n",
    "import pandas as pd\n",
    "pd.Datarame(wc, columns=['mots', 'frequence'])\n",
    "df[df['frequence'] > 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tok = Tokenizer()\n",
    "test_tok.fit_on_texts(test_sentences)\n",
    "test_words = test_tok.word_index.keys()\n",
    "train_words = df['mots'].tolist()\n",
    "len(train_words), len(test_words)\n",
    "\n",
    "inter = set(train_words).intersection(set(test_words)) #tout les mots en commun\n",
    "len(inter) # 33'000 mots en commun\n",
    "vocab_size = 29000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser racine 4eme du vocab_size pour la dimension de l'embedding\n",
    "embedding_dim = np.power(vocab_size, 1/4) # ~13\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequentials(\n",
    "\t[\n",
    "\t\ttf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "\t\ttf.keras.layers.GlobalAveragePooling1D(),\n",
    "\t\ttf.keras.layers.Dense(5,activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(3, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\t])\n",
    "\n",
    "# plus longtemps\n",
    "# plus de mots\n",
    "# optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change taille maximal phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on coupe les phrases en max 15 mots\n",
    "tailles = []\n",
    "for sent in training_sentences:\n",
    "\ttailles.append(len(sent.split(\" \")))\n",
    "np.array(tailles).mean() # 233 mots par phrase\n",
    "np.array(tailles).max() # 2470 mots par phrase\n",
    "np.array(tailles).min() # 4 mots par phrase\n",
    "np.array(tailles).median() # 174 mots par phrase\n",
    "# adapter maxlen = 100 au lieu de 15\n",
    "# rajouter aussi du dropout (randomly drop some neurons to prevent overfitting)\n",
    "model = tf.keras.models.Sequentials(\n",
    "\t[\n",
    "\t\tembded\n",
    "\t\ttf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "\t\ttf.keras.layers.GlobalAveragePooling1D(),\n",
    "\t\ttf.keras.layers.Dense(5,activation='relu'),\n",
    "\t\ttf.keras.layers.Dropout(0.25),\n",
    "\t\ttf.keras.layers.Dense(3, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\t])\n",
    "# le resultat est pertinant , on a un meilleur model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfert learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow.org/hub\n",
    "!pip install --upgrade tensorflow-hub\n",
    "# utilise 130GB de corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\")\n",
    "embeddings = embed([\"A long sentence.\", \"single-word\", )\n",
    "embeddings.shape # (2, 20) 2 phrases, 20 dimensions\n",
    "\n",
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\",\n",
    "\t\t\t\t\t\t\tinput_shape=[],\n",
    "\t\t\t\t\t\t\tdtype=tf.string,\n",
    "\t\t\t\t\t\t\ttrainable=True)\n",
    "\n",
    "train_examples = tfds.as_numpy(training_data)\n",
    "\n",
    "model = tf.keras.models.Sequentials(\n",
    "\t[\n",
    "\t\thub_layer,\n",
    "\t\ttf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "\t\ttf.keras.layers.GlobalAveragePooling1D(),\n",
    "\t\ttf.keras.layers.Dense(5,activation='relu'),\n",
    "\t\ttf.keras.layers.Dropout(0.25),\n",
    "\t\ttf.keras.layers.Dense(3, activation='relu'),\n",
    "\t\ttf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser Embeddings projector pour se representer\n",
    "- PCA en 3D pour savoir les mots les plus proches en 3D\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
